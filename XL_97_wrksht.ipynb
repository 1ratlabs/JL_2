{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cbb72ef-b02f-48c2-a659-88d6c3fee5e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Importing from xl \"ib97\" to get \"df_mf_ib97_nn_s\"  & xl \"ib97\" to get \"df_mf_ib97_nn_s\"** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98299ccf-3813-4908-b2cb-110b5d26dd77",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc44cd5a-40d5-4bff-8dde-de21b6877ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bhuns/miniconda3/bin/python\n",
      "note: THIS IS THE DIRECTORY PYTHON IS WORKING IN.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(\"note: THIS IS THE DIRECTORY PYTHON IS WORKING IN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e1c6941-4bbc-421d-bcf4-ee3bb2055a94",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all imports loaded\n"
     ]
    }
   ],
   "source": [
    "# Imports required for Loading, sorting .csv files to create specific data sets ie mrn inbody readings. \n",
    "%run ./sys_funcs.py              # loads all the def functions in sys_funcs.py into memory\n",
    "#import sys_funcs                 # gives access to these def function digitalform that are in memory\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from datetime import time\n",
    "from sys_funcs import read_csv_to_array\n",
    "from sys_funcs import clean_wsl_path\n",
    "from sys_funcs import array_to_dt_row_dict\n",
    "from sys_funcs import make_blnk_update_row_dict\n",
    "from sys_funcs import transpose_csv_to_col_dict\n",
    "#from sys_funcs import update_values_with_config, get_update_result\n",
    "from sys_funcs import transfer_updates\n",
    "from sys_funcs import get_dtv_range\n",
    "from sys_funcs import universal_import\n",
    "from sys_funcs import parse_inbody_timestamp\n",
    "from sys_funcs import build_lut\n",
    "from sys_funcs import extract_a_column_as_df\n",
    "from sys_funcs import extract_multicolumns_as_df\n",
    "from sys_funcs import validate_and_sort_timestamps\n",
    "from sys_funcs import extract_and_filter_by_time_window\n",
    "from sys_funcs import read_file_dual_path\n",
    "from sys_funcs import write_file_dual_path\n",
    "from sys_funcs import asc_to_csv_cnv\n",
    "from collections.abc import Mapping\n",
    "import re\n",
    "#from sys_funcs import\n",
    "print(\"all imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40073c29-4be1-4d3f-86a4-f44d70c3d61f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print set to 1000 rows max\n"
     ]
    }
   ],
   "source": [
    "# set print rows  This worksheet sets maximum # of rows printed\n",
    "pd.set_option('display.max_rows', 1000)  # Adjust the number of rows to display\n",
    "# pd.reset_option('display.max_rows')  \n",
    "print('print set to 1000 rows max' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c780f0dd-0f8b-4742-9c02-a5a9512f19bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: timestamp = Test Date / Time does not work  use computed time stamp\n"
     ]
    }
   ],
   "source": [
    "print(\"NOTE: timestamp = Test Date / Time does not work  use computed time stamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acfc2e0-7d7c-44d0-8b42-b7b653431032",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Def functions called in data importing & refinment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ad5ac93-e82e-450b-9b97-4b49dc414104",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "media_lst = [\n",
    "    \"timestamp\",\n",
    "    \"dtv\",\n",
    "    \"ib_id\",\n",
    "    \"cls\",\n",
    "    \"cmmnts\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "152dd983-4fa2-4fc8-a108-3bc04d9dea4a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3rd version def drop_duplicates_by_test_time(df, keep='first', log=True):\n",
    "def drop_duplicates_by_test_time(df, keep='first', log=True):\n",
    "    \"\"\"\n",
    "    Removes duplicate rows based on the 'Test Date / Time' column.\n",
    "    Keeps only the first (or last) occurrence.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Identify duplicate timestamps (beyond the one we keep)\n",
    "    dupes = (\n",
    "        df.loc[df.duplicated(subset=['Test Date / Time'], keep=keep), 'Test Date / Time']\n",
    "        .astype(str)\n",
    "        .values\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    # Drop duplicate rows\n",
    "    df = df.drop_duplicates(subset=['Test Date / Time'], keep=keep)\n",
    "\n",
    "    # Optional logging\n",
    "    if log and dupes:\n",
    "        print(\"Removed duplicate rows for timestamps:\", dupes)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e73aa88c-6a17-41b0-ab8d-265ea995135d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def strip_numbers_from_columns(df):\n",
    "import re\n",
    "\n",
    "def strip_numbers_from_columns(df):\n",
    "    \"\"\"\n",
    "    Removes leading/trailing numbers and any leftover separators\n",
    "    so that cases like '1.0ID' become 'ID'.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    new_cols = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        cleaned = col\n",
    "\n",
    "        # Remove leading numbers + separators\n",
    "        cleaned = re.sub(r'^\\d+[\\s\\-\\_\\.:]*', '', cleaned)\n",
    "\n",
    "        # Remove trailing numbers + separators\n",
    "        cleaned = re.sub(r'[\\s\\-\\_\\.:]*\\d+$', '', cleaned)\n",
    "\n",
    "        # Remove leftover leading/trailing punctuation (.,-_:) after number removal\n",
    "        cleaned = re.sub(r'^[\\.\\-\\_\\:]+', '', cleaned)\n",
    "        cleaned = re.sub(r'[\\.\\-\\_\\:]+$', '', cleaned)\n",
    "\n",
    "        new_cols[col] = cleaned\n",
    "\n",
    "    return df.rename(columns=new_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eddfc987-92f4-4957-a631-a335c66df082",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Use drop duplicate function If duplicates are found\n",
    "def drop_duplicate_columns(df, keep='first', log=True):\n",
    "    \"\"\"\n",
    "    Removes duplicate column names from a DataFrame, keeping only the first\n",
    "    (or last) occurrence. Useful after column-cleaning steps that may cause\n",
    "    collisions. good i'm moving this around because I want to go ahead and do the I'm talking too\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Input DataFrame.\n",
    "    keep : {'first', 'last'}, default 'first'\n",
    "        Which duplicate to keep.\n",
    "    log : bool, default True\n",
    "        Whether to print which columns were removed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with duplicate columns removed.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Identify duplicates beyond the one we keep\n",
    "    dupes = df.columns[df.columns.duplicated(keep=keep)].tolist()\n",
    "\n",
    "    # Drop them\n",
    "    df = df.loc[:, ~df.columns.duplicated(keep=keep)]\n",
    "\n",
    "    # Optional logging\n",
    "    if log and dupes:\n",
    "        print(\"Removed duplicate columns:\", dupes)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2a70e40-66fb-4e3a-a45e-2bb221d52eea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def prepend_empty_columns(df, col_list):\n",
    "def prepend_empty_columns(df, col_list):\n",
    "    \"\"\"\n",
    "    Prepend empty columns (from col_list) to the front of df.\n",
    "    Returns a new DataFrame.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    # Create empty columns with same row count\n",
    "    empty_df = pd.DataFrame(\n",
    "        {col: [None] * len(df) for col in col_list}\n",
    "    )\n",
    "\n",
    "    # Prepend them\n",
    "    return pd.concat([empty_df, df], axis=1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "275e0f36-1c1c-4582-9144-f6289464ed57",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# old raw def fill_ib_media_cols(df):\n",
    "#filling the media cols\n",
    "def fill_ib_media_cols(df):\n",
    "    \"\"\"\n",
    "    Fills the 5 leading operator columns for InBody datasets:\n",
    "      - timestamp  ‚Üê parsed from 'Test Date / Time' (YYYYMMDDHHMMSS)\n",
    "      - dtv        ‚Üê days since 1900‚Äë01‚Äë01\n",
    "      - ib_id      ‚Üê 'mrn' if test time 03:00‚Äì23:59, else 'eve'\n",
    "      - cls        ‚Üê NaN\n",
    "      - cmmnts     ‚Üê NaN\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- 1. timestamp (correct parsing) ------------------------\n",
    "    df['timestamp'] = pd.to_datetime(\n",
    "        df['Test Date / Time'].astype(str),\n",
    "        format=\"%Y%m%d%H%M%S\",\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # --- 2. dtv: days since 1900‚Äë01‚Äë01 -------------------------\n",
    "    origin = pd.Timestamp(\"1900-01-01\")\n",
    "    df['dtv'] = (df['timestamp'] - origin).dt.days\n",
    "\n",
    "    # --- 3. ib_id classification -------------------------------\n",
    "    def classify_ib_id(ts):\n",
    "        if pd.isna(ts):\n",
    "            return np.nan\n",
    "        hour = ts.hour\n",
    "        return \"mrn\" if 3 <= hour <= 12 else \"eve\"\n",
    "\n",
    "    df['ib_id'] = df['timestamp'].apply(classify_ib_id)\n",
    "\n",
    "    # --- 4. cls ------------------------------------------------\n",
    "    df['cls'] = np.nan\n",
    "\n",
    "    # --- 5. cmmnts ---------------------------------------------\n",
    "    df['cmmnts'] = np.nan\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54c8b4e1-7019-4f5e-8bc4-a2dbeabeb08b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# revised def fill_ib_media_cols(df):\n",
    "def fill_ib_media_cols(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- 1. Clean and parse timestamp --------------------------\n",
    "    def fix_ts(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        # Convert float ‚Üí int safely\n",
    "        try:\n",
    "            x_int = int(float(x))\n",
    "        except:\n",
    "            return np.nan\n",
    "        # Zero‚Äëpad to 14 digits (YYYYMMDDHHMMSS)\n",
    "        s = str(x_int).zfill(14)\n",
    "        return pd.to_datetime(s, format=\"%Y%m%d%H%M%S\", errors=\"coerce\")\n",
    "\n",
    "    df['timestamp'] = df['Test Date / Time'].apply(fix_ts)\n",
    "\n",
    "    # --- 2. dtv ------------------------------------------------\n",
    "    origin = pd.Timestamp(\"1900-01-01\")\n",
    "    df['dtv'] = (df['timestamp'] - origin).dt.days\n",
    "\n",
    "    # --- 3. ib_id ----------------------------------------------\n",
    "    def classify_ib_id(ts):\n",
    "        if pd.isna(ts):\n",
    "            return np.nan\n",
    "        hour = ts.hour\n",
    "        return \"mrn\" if 3 <= hour <= 12 else \"eve\"\n",
    "\n",
    "    df['ib_id'] = df['timestamp'].apply(classify_ib_id)\n",
    "\n",
    "    # --- 4‚Äì5. cls, cmmnts --------------------------------------\n",
    "    df['cls'] = np.nan\n",
    "    df['cmmnts'] = np.nan\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13c4627e-8ec3-4306-8d0c-2af47e7975d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Sort the rows by timestamp\n",
    "def sort_by_timestamp(df):\n",
    "    \"\"\"\n",
    "    Sorts an InBody dataframe by the 'timestamp' column\n",
    "    in ascending chronological order.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(by='timestamp', ascending=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a9b43c2-dace-4c30-beeb-97667a3f6099",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# A function to combine frames of ib97 with IB97 and sorting into one data frame. N\n",
    "# this will be used on a column by column basis for a list of columns.\n",
    "def combine_weight_frames(df_a, df_b, ts_col=\"timestamp\", wt_a=\"2. wt\", wt_b=\"4. wt\"):\n",
    "    \"\"\"\n",
    "    Combines two dataframes with different weight column names into a single\n",
    "    dataframe with columns: timestamp, wt, sorted by timestamp.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_a : pd.DataFrame\n",
    "        First dataframe containing a timestamp column and a weight column.\n",
    "    df_b : pd.DataFrame\n",
    "        Second dataframe containing a timestamp column and a weight column.\n",
    "    ts_col : str, optional\n",
    "        Name of the timestamp column (default 'timestamp').\n",
    "    wt_a : str, optional\n",
    "        Weight column name in df_a (default '2. wt').\n",
    "    wt_b : str, optional\n",
    "        Weight column name in df_b (default '4. wt').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Combined dataframe with columns: timestamp, wt, sorted by timestamp.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize df_a\n",
    "    df_a_norm = df_a[[ts_col, wt_a]].rename(columns={wt_a: \"wt\"})\n",
    "\n",
    "    # Normalize df_b\n",
    "    df_b_norm = df_b[[ts_col, wt_b]].rename(columns={wt_b: \"wt\"})\n",
    "\n",
    "    # Stack them vertically\n",
    "    df_combined = pd.concat([df_a_norm, df_b_norm], ignore_index=True)\n",
    "\n",
    "    # Sort by timestamp\n",
    "    df_combined = df_combined.sort_values(by=ts_col).reset_index(drop=True)\n",
    "\n",
    "    return df_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e38e464-cbb8-403c-9009-39184945cc3f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def drop_duplicate_source_files(df, keep=\"first\"):\n",
    "def drop_duplicate_source_files(df, keep=\"first\"):\n",
    "    \"\"\"\n",
    "    Remove rows where 'source_file' appears more than once.\n",
    "    keep='first' keeps the first occurrence,\n",
    "    keep='last' keeps the last occurrence.\n",
    "    \"\"\"\n",
    "    if \"source_file\" not in df.columns:\n",
    "        raise ValueError(\"Column 'source_file' not found in DataFrame\")\n",
    "\n",
    "    return df.drop_duplicates(subset=[\"source_file\"], keep=keep).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7eab1d0a-6cac-4215-91cc-b6f15049d11b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# remove ROW duplicates on the basis of timestamp\n",
    "def remove_ib_duplicates(df, subset_cols=None):\n",
    "    \"\"\"\n",
    "    Removes duplicate InBody rows based on key identifying columns.\n",
    "    Default behavior: remove duplicates based on ['ID', 'timestamp'].\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Default duplicate definition\n",
    "    if subset_cols is None:\n",
    "        subset_cols = ['timestamp']\n",
    "        # subset_cols = ['ID', 'timestamp']\n",
    "    # Remove duplicates, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset=subset_cols, keep='first')\n",
    "\n",
    "    # Reset index for cleanliness\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90e29752-3a38-42e8-a014-06347bdfd08b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# remove ROW duplicates on the basis of \"Test Date / Time\"\n",
    "def remove_ib_duplicates_Test_Date_Time(df, subset_cols=None):\n",
    "    \"\"\"\n",
    "    Removes duplicate InBody rows based on key identifying columns.\n",
    "    Default behavior: remove duplicates based on ['ID', 'Test Date / Time'].\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Default duplicate definition\n",
    "    if subset_cols is None:\n",
    "        subset_cols = ['Test Date / Time']\n",
    "        # subset_cols = ['ID', 'Test Date / Time']\n",
    "    # Remove duplicates, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset=subset_cols, keep='first')\n",
    "\n",
    "    # Reset index for cleanliness\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6f8def7-1c3d-4df6-b53a-0d3ea6978f0e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# veirfy if rows exixt in master_timestamps(df_master, df_new, ts_col=\"timestamp\"):\n",
    "def filter_new_rows_by_master_timestamps(df_master, df_new, ts_col=\"timestamp\"):\n",
    "    \"\"\"\n",
    "    Filters df_new so that only rows whose timestamps appear in df_master remain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_master : pd.DataFrame\n",
    "        The master dataframe containing valid timestamps.\n",
    "    df_new : pd.DataFrame\n",
    "        The new dataframe to be filtered.\n",
    "    ts_col : str, optional\n",
    "        The name of the timestamp column (default is 'timestamp').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A filtered version of df_new containing only rows whose timestamps\n",
    "        exist in df_master.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the set of valid timestamps from the master dataframe\n",
    "    valid_timestamps = set(df_master[ts_col])\n",
    "\n",
    "    # Filter df_new to keep only rows with timestamps in the master set\n",
    "    df_filtered = df_new[df_new[ts_col].isin(valid_timestamps)].copy()\n",
    "\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25e2d5e7-641f-4c83-ae6a-33ea309cb874",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def keep_only_new_timestamps(df_master, df_new, ts_col=\"timestamp\")\n",
    "def keep_only_new_timestamps(df_master, df_new, ts_col=\"timestamp\"):\n",
    "    \"\"\"\n",
    "    Returns only the rows in df_new whose timestamps do NOT exist in df_master.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_master : pd.DataFrame\n",
    "        The master dataframe containing timestamps already ingested.\n",
    "    df_new : pd.DataFrame\n",
    "        The new dataframe to be filtered.\n",
    "    ts_col : str, optional\n",
    "        The name of the timestamp column (default is 'timestamp').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A filtered version of df_new containing only rows with timestamps\n",
    "        NOT present in df_master.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the set of timestamps already in the master\n",
    "    existing_ts = set(df_master[ts_col])\n",
    "\n",
    "    # Keep only rows whose timestamp is NOT in the master\n",
    "    df_filtered = df_new[~df_new[ts_col].isin(existing_ts)].copy()\n",
    "\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb376852-d4e8-4f86-813a-3b09388513fd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def append_rows_with_master_schema(master_df, adder_df):\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def append_rows_with_master_schema(master_df, adder_df):\n",
    "    \"\"\"\n",
    "    Appends rows from adder_df into master_df while enforcing the master_df schema.\n",
    "\n",
    "    For each row in adder_df:\n",
    "      - Columns that exist in adder_df are copied.\n",
    "      - Columns missing from adder_df are filled with NaN.\n",
    "      - All master_df columns are preserved in order.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    master_df : pd.DataFrame\n",
    "        The master dataframe with the full schema.\n",
    "    adder_df : pd.DataFrame\n",
    "        The dataframe containing rows to append (subset of master columns).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Updated master_df with new rows appended.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reindex adder_df to match master_df columns, filling missing columns with NaN\n",
    "    adder_aligned = adder_df.reindex(columns=master_df.columns)\n",
    "\n",
    "    # Append and return\n",
    "    return pd.concat([master_df, adder_aligned], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "052ecc49-53e1-431c-8cd9-be72ca600e80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def filter_by_value(df, column, value):\n",
    "def filter_by_value(df, column, value):\n",
    "    \"\"\"\n",
    "    Returns a filtered DataFrame containing only rows where df[column] == value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The dataframe to filter.\n",
    "    column : str\n",
    "        The column name to filter on.\n",
    "    value : any\n",
    "        The value that the column must match.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A filtered dataframe containing only matching rows.\n",
    "    \"\"\"\n",
    "    return df[df[column] == value].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "399b4af2-bb01-4c5c-bbc8-b10d524bafd6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def write_df_to_pickle(df, filename):\n",
    "def write_df_to_pickle(df, filename):\n",
    "    \"\"\"\n",
    "    Writes a DataFrame to a pickle file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The dataframe to save.\n",
    "    filename : str\n",
    "        The pickle filename, e.g. 'mydata.pkl'.\n",
    "    \"\"\"\n",
    "    df.to_pickle(filename)\n",
    "\n",
    "# usage \n",
    "# write_df_to_pickle(df, \"df.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31a7611d-333d-4bd8-931b-4251d5eefada",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def load_df_from_pickle(filename):\n",
    "\n",
    "def load_df_from_pickle(filename):\n",
    "    \"\"\"\n",
    "    Loads a DataFrame from a pickle file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Path to the pickle file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "    \"\"\"\n",
    "    return pd.read_pickle(filename)\n",
    "\n",
    "    # usage \n",
    "    # df = load_df_from_pickle(\"df.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65cee1d-c271-4489-b29a-daaeb5fa5811",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def scale_mean_to_one(series):\n",
    "def scale_mean_to_one(series):\n",
    "    \"\"\"Scale a Pandas Series so that its mean becomes 1.\"\"\"\n",
    "    mean_val = series.mean()\n",
    "    return series / mean_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4a8e1e-54bd-4261-ac7e-efca87d980c2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def plot_column(df, col_name):\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_column(df, col_name):\n",
    "    \"\"\"\n",
    "    Plot a single column from a dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing the column.\n",
    "    col_name : str\n",
    "        The name of the column to plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(df[col_name], marker='o', linestyle='-', linewidth=1)\n",
    "    plt.title(f\"{col_name} over index\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(col_name)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c27f01-755b-4487-bc97-555761bef186",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Universal csv loader that can accept all csv encoders\n",
    "# def universal_import(folder_path,\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "def universal_import(\n",
    "    folder_path,\n",
    "    pattern=\"*.csv\",\n",
    "    expected_columns=None,\n",
    "    df_name=None,\n",
    "    verbose=True\n",
    "):\n",
    "    folder = Path(folder_path)\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Folder not found: {folder_path}\")\n",
    "\n",
    "    dfs = []\n",
    "    #files = list(folder.glob(pattern))\n",
    "    #files = [f for f in folder.glob(pattern) if not f.name.endswith(\".utf8.csv\")]\n",
    "    files = [\n",
    "        f for f in folder.glob(pattern)\n",
    "        if not f.name.lower().endswith(\".utf8.csv\")\n",
    "           and not f.name.lower().endswith(\"_utf8.csv\")\n",
    "]\n",
    "\n",
    "\n",
    "    for f in files:\n",
    "        if verbose:\n",
    "            print(f\"\\nüìÑ Processing: {f.name}\")\n",
    "\n",
    "        # --- 1. Detect encoding ---\n",
    "        try:\n",
    "            with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "                sample = fh.read(4096)\n",
    "            encoding_used = \"utf-8\"\n",
    "        except Exception:\n",
    "            encoding_used = \"ISO-8859-1\"\n",
    "\n",
    "        # --- 2. Detect delimiter ---\n",
    "        try:\n",
    "            with open(f, \"r\", encoding=encoding_used) as fh:\n",
    "                sample = fh.read(4096)\n",
    "                dialect = csv.Sniffer().sniff(sample)\n",
    "                delimiter = dialect.delimiter\n",
    "        except Exception:\n",
    "            delimiter = \",\"  # safe fallback\n",
    "\n",
    "        # --- 3. Load CSV ---\n",
    "        try:\n",
    "            df = pd.read_csv(f, encoding=encoding_used, sep=delimiter)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"‚ùå Failed to load {f.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # --- 4. Schema validation (optional) ---\n",
    "        if expected_columns and df.shape[1] != expected_columns:\n",
    "            if verbose:\n",
    "                print(f\"‚ö†Ô∏è Skipped {f.name}: expected {expected_columns} cols, got {df.shape[1]}\")\n",
    "            continue\n",
    "\n",
    "        # --- 5. Normalize to UTF-8 for future stability ---\n",
    "        utf8_path = f.with_suffix(\".utf8.csv\")\n",
    "        df.to_csv(utf8_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "        # --- 6. Add metadata ---\n",
    "        #df[\"source_file\"] = f.name\n",
    "        #df[\"encoding_used\"] = encoding_used\n",
    "        #df[\"delimiter_used\"] = delimiter\n",
    "        # --- 6. Add metadata safely ---\n",
    "        if \"source_file\" not in df.columns:\n",
    "            df[\"source_file\"] = f.name\n",
    "        \n",
    "        if \"encoding_used\" not in df.columns:\n",
    "            df[\"encoding_used\"] = encoding_used\n",
    "        \n",
    "        if \"delimiter_used\" not in df.columns:\n",
    "            df[\"delimiter_used\"] = delimiter\n",
    "\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"‚úÖ Loaded {f.name} | encoding={encoding_used} | delimiter='{delimiter}'\")\n",
    "            print(f\"üíæ Normalized UTF-8 saved as: {utf8_path.name}\")\n",
    "\n",
    "    # --- 7. Combine all files ---\n",
    "    if not dfs:\n",
    "        if verbose:\n",
    "            print(\"\\nüö´ No valid files loaded.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # --- 8. Save combined pickle ---\n",
    "    label = df_name if df_name else \"imported_dataframe\"\n",
    "    pickle_path = Path.cwd() / f\"{label}.pkl\"\n",
    "    combined.to_pickle(pickle_path)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nüì¶ Combined DataFrame: {len(combined)} rows from {len(dfs)} files.\")\n",
    "        print(f\"üíæ Saved combined pickle: {pickle_path}\")\n",
    "\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed12b773-8240-4b46-8a7f-2300b7d65f79",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# NOTE this works only for load_ib97_folder(folder_path)\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_ib97_folder(folder_path):\n",
    "    folder = Path(folder_path)\n",
    "    dfs = []\n",
    "\n",
    "    for f in folder.glob(\"*.csv\"):\n",
    "        try:\n",
    "            df = pd.read_csv(f, encoding=\"latin1\", sep=\",\")\n",
    "            df[\"source_file\"] = f.name\n",
    "            # print(df[\"Test Date / Time\"])\n",
    "            dfs.append(df)\n",
    "            print(f\"Loaded: {f.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {f.name}: {e}\")\n",
    "\n",
    "    if not dfs:\n",
    "        print(\"No valid CSV files found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    ib97_raw = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"\\nFinal DataFrame: {len(ib97_raw)} rows from {len(dfs)} files.\")\n",
    "    return ib97_raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09485323-85e1-4315-bb9a-32c5d12c0e5a",
   "metadata": {},
   "source": [
    "# Import new data from ip77 in the data folder of repo and results =  \"df_mf_ib97_nn_s\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6020b827-978e-400b-b6e5-60181dc345af",
   "metadata": {},
   "source": [
    "## Creating \"df_mf_ib97_nn_s\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dcdaf7-bf31-4afd-8c57-2f0d3336156e",
   "metadata": {},
   "source": [
    "### This segment imports the data from the Excel file probo/data/ib97 to dataframe with numbers in col names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52fcdc0e-517c-455b-a918-b646e713c7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Skipped Master ib970 Exports.xlsm: Error tokenizing data. C error: Expected 1 fields in line 9, saw 2\n",
      "\n",
      "‚úÖ Loaded 251201-1_20251204064205.utf8.csv with utf-8\n",
      "‚úÖ Loaded 251201-1_20251204064205.csv with ISO-8859-1\n",
      "‚úÖ [imported_dataframe] Final DataFrame: 2 rows from 2 files.\n",
      "üíæ Saved to pickle: /home/bhuns/JL_2/imported_dataframe.pkl\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"/home/bhuns/JL_2/data/ib97\"\n",
    "df_ib97_raw = universal_import(folder_path, pattern=\"*\", expected_columns=None, df_name=None, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ae4c539-78cc-40f6-b461-ae3df116e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify df_ib97_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05d0dfc3-ef01-44fa-a2a0-501c4e0a019c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Name',\n",
       " '2. ID',\n",
       " '3. Height',\n",
       " '4. Date of Birth',\n",
       " '5. Gender',\n",
       " '6. Age',\n",
       " '7. Mobile Number',\n",
       " '8. Phone Number',\n",
       " '9. Zip Code',\n",
       " '10. Address',\n",
       " '11. E-mail',\n",
       " '12. Date of Registration',\n",
       " '13. Memo',\n",
       " '14. Test Date / Time',\n",
       " '15. Weight',\n",
       " '16. Lower Limit (Weight Normal Range)',\n",
       " '17. Upper Limit (Weight Normal Range)',\n",
       " '18. TBW (Total Body Water)',\n",
       " '19. Lower Limit (TBW Normal Range)',\n",
       " '20. Upper Limit (TBW Normal Range)',\n",
       " '21. ICW (Intracellular Water)',\n",
       " '22. Lower Limit (ICW Normal Range)',\n",
       " '23. Upper Limit (ICW Normal Range)',\n",
       " '24. ECW (Extracellular Water)',\n",
       " '25. Lower Limit (ECW Normal Range)',\n",
       " '26. Upper Limit (ECW Normal Range)',\n",
       " '27. Protein',\n",
       " '28. Lower Limit (Protein Normal Range)',\n",
       " '29. Upper Limit (Protein Normal Range)',\n",
       " '30. Minerals',\n",
       " '31. Lower Limit (Minerals Normal Range)',\n",
       " '32. Upper Limit (Minerals Normal Range)',\n",
       " '33. DLM (Dry Lean Mass)',\n",
       " '34. BFM (Body Fat Mass)',\n",
       " '35. Lower Limit (BFM Normal Range)',\n",
       " '36. Upper Limit (BFM Normal Range)',\n",
       " '37. FFM (Fat Free Mass)',\n",
       " '38. SMM (Skeletal Muscle Mass)',\n",
       " '39. Lower Limit (SMM Normal Range)',\n",
       " '40. Upper Limit (SMM Normal Range)',\n",
       " '41. BMI (Body Mass Index)',\n",
       " '42. Lower Limit (BMI Normal Range)',\n",
       " '43. Upper Limit (BMI Normal Range)',\n",
       " '44. PBF (Percent Body Fat)',\n",
       " '45. Lower Limit (PBF Normal Range)',\n",
       " '46. Upper Limit (PBF Normal Range)',\n",
       " '47. Lean Mass of Right Arm',\n",
       " '48. Lower Limit (Lean Mass of Right Arm Normal Range)',\n",
       " '49. Upper Limit (Lean Mass of Right Arm Normal Range)',\n",
       " '50. Lean Mass(%) of Right Arm',\n",
       " '51. Lean Mass of Left Arm',\n",
       " '52. Lower Limit (Lean Mass of Left Arm Normal Range)',\n",
       " '53. Upper Limit (Lean Mass of Left Arm Normal Range)',\n",
       " '54. Lean Mass(%) of Left Arm',\n",
       " '55. Lean Mass of Trunk',\n",
       " '56. Lower Limit (Lean Mass of Trunk Normal Range)',\n",
       " '57. Upper Limit (Lean Mass of Trunk Normal Range)',\n",
       " '58. Lean Mass(%) of Trunk',\n",
       " '59. Lean Mass of Right Leg',\n",
       " '60. Lower Limit (Lean Mass of Right Leg Normal Range)',\n",
       " '61. Upper Limit (Lean Mass of Right Leg Normal Range)',\n",
       " '62. Lean Mass(%) of Right Leg',\n",
       " '63. Lean Mass of Left Leg',\n",
       " '64. Lower Limit (Lean Mass of Left Leg Normal Range)',\n",
       " '65. Upper Limit (Lean Mass of Left Leg Normal Range)',\n",
       " '66. Lean Mass(%) of Left Leg',\n",
       " '67. Leg Lean Mass',\n",
       " '68. TBW of Right Arm',\n",
       " '69. Lower Limit (TBW of Right Arm Normal Range)',\n",
       " '70. Upper Limit (TBW of Right Arm Normal Range)',\n",
       " '71. TBW of Left Arm',\n",
       " '72. Lower Limit (TBW of Left Arm Normal Range)',\n",
       " '73. Upper Limit (TBW of Left Arm Normal Range)',\n",
       " '74. TBW of Trunk',\n",
       " '75. Lower Limit (TBW of Trunk Normal Range)',\n",
       " '76. Upper Limit (TBW of Trunk Normal Range)',\n",
       " '77. TBW of Right Leg',\n",
       " '78. Lower Limit (TBW of Right Leg Normal Range)',\n",
       " '79. Upper Limit (TBW of Right Leg Normal Range)',\n",
       " '80. TBW of Left Leg',\n",
       " '81. Lower Limit (TBW of Left Leg Normal Range)',\n",
       " '82. Upper Limit (TBW of Left Leg Normal Range)',\n",
       " '83. ICW of Right Arm',\n",
       " '84. Lower Limit (ICW of Right Arm Normal Range)',\n",
       " '85. Upper Limit (ICW of Right Arm Normal Range)',\n",
       " '86. ICW of Left Arm',\n",
       " '87. Lower Limit (ICW of Left Arm Normal Range)',\n",
       " '88. Upper Limit (ICW of Left Arm Normal Range)',\n",
       " '89. ICW of Trunk',\n",
       " '90. Lower Limit (ICW of Trunk Normal Range)',\n",
       " '91. Upper Limit (ICW of Trunk Normal Range)',\n",
       " '92. ICW of Right Leg',\n",
       " '93. Lower Limit (ICW of Right Leg Normal Range)',\n",
       " '94. Upper Limit (ICW of Right Leg Normal Range)',\n",
       " '95. ICW of Left Leg',\n",
       " '96. Lower Limit (ICW of Left Leg Normal Range)',\n",
       " '97. Upper Limit (ICW of Left Leg Normal Range)',\n",
       " '98. ECW of Right Arm',\n",
       " '99. Lower Limit (ECW of Right Arm Normal Range)',\n",
       " '100. Upper Limit (ECW of Right Arm Normal Range)',\n",
       " '101. ECW of Left Arm',\n",
       " '102. Lower Limit (ECW of Left Arm Normal Range)',\n",
       " '103. Upper Limit (ECW of Left Arm Normal Range)',\n",
       " '104. ECW of Trunk',\n",
       " '105. Lower Limit (ECW of Trunk Normal Range)',\n",
       " '106. Upper Limit (ECW of Trunk Normal Range)',\n",
       " '107. ECW of Right Leg',\n",
       " '108. Lower Limit (ECW of Right Leg Normal Range)',\n",
       " '109. Upper Limit (ECW of Right Leg Normal Range)',\n",
       " '110. ECW of Left Leg',\n",
       " '111. Lower Limit (ECW of Left Leg Normal Range)',\n",
       " '112. Upper Limit (ECW of Left Leg Normal Range)',\n",
       " '113. ECW/TBW',\n",
       " '114. ECW/TBW of Right Arm',\n",
       " '115. ECW/TBW of Left Arm',\n",
       " '116. ECW/TBW of Trunk',\n",
       " '117. ECW/TBW of Right Leg',\n",
       " '118. ECW/TBW of Left Leg',\n",
       " '119. BFM of Right Arm',\n",
       " '120. BFM% of Right Arm',\n",
       " '121. BFM of Left Arm',\n",
       " '122. BFM% of Left Arm',\n",
       " '123. BFM of Trunk',\n",
       " '124. BFM% of Trunk',\n",
       " '125. BFM of Right Leg',\n",
       " '126. BFM% of Right Leg',\n",
       " '127. BFM of Left Leg',\n",
       " '128. BFM% of Left Leg',\n",
       " '129. InBody Score',\n",
       " '130. BFM Control',\n",
       " '131. FFM Control',\n",
       " '132. BMR (Basal Metabolic Rate)',\n",
       " '133. VFL (Visceral Fat Level)',\n",
       " '134. VFA (Visceral Fat Area)',\n",
       " '135. BCM (Body Cell Mass)',\n",
       " '136. Lower Limit (BCM Normal Range)',\n",
       " '137. Upper Limit (BCM Normal Range)',\n",
       " '138. AC (Arm Circumference)',\n",
       " '139. BMC (Bone Mineral Content)',\n",
       " '140. Lower Limit (BMC Normal Range)',\n",
       " '141. Upper Limit (BMC Normal Range)',\n",
       " '142. TBW/FFM',\n",
       " '143. FFMI (Fat Free Mass Index)',\n",
       " '144. FMI (Fat Mass Index)',\n",
       " '145. 5kHz-RA Phase Angle',\n",
       " '146. 5kHz-LA Phase Angle',\n",
       " '147. 5kHz-TR Phase Angle',\n",
       " '148. 5kHz-RL Phase Angle',\n",
       " '149. 5kHz-LL Phase Angle',\n",
       " '150. 50kHz-RA Phase Angle',\n",
       " '151. 50kHz-LA Phase Angle',\n",
       " '152. 50kHz-TR Phase Angle',\n",
       " '153. 50kHz-RL Phase Angle',\n",
       " '154. 50kHz-LL Phase Angle',\n",
       " '155. 250kHz-RA Phase Angle',\n",
       " '156. 250kHz-LA Phase Angle',\n",
       " '157. 250kHz-TR Phase Angle',\n",
       " '158. 250kHz-RL Phase Angle',\n",
       " '159. 250kHz-LL Phase Angle',\n",
       " '160. 50kHz-Whole Body Phase Angle',\n",
       " '161. Systolic',\n",
       " '162. Diastolic',\n",
       " '163. Pulse',\n",
       " '164. Mean Artery Pressure',\n",
       " '165. Pulse Pressure',\n",
       " '166. Rate Pressure Product',\n",
       " '167. InBody Type',\n",
       " '168. Local ID',\n",
       " '169. ICW/FFM%',\n",
       " '170. SMI (Skeletal Muscle Index)',\n",
       " '171. Medical History',\n",
       " '172. Group',\n",
       " '173. Recommended Calorie Intake',\n",
       " '174. Lower Limit (BMR Normal Range)',\n",
       " '175. Upper Limit (BMR Normal Range)',\n",
       " '176. SMI(ASM/Ht¬≤)_T score',\n",
       " '177. SMI(ASM/Ht¬≤)_Z score',\n",
       " '178. Whole Body ECW/TBW_T score',\n",
       " '179. Whole Body ECW/TBW_Z score',\n",
       " '180. VFA_T score',\n",
       " '181. VFA_Z score',\n",
       " '182. BMI_T score',\n",
       " '183. BMI_Z score',\n",
       " '184. Subcutaneous Fat',\n",
       " '185. Arms/Legs Fat',\n",
       " '186. Lower Limit (Subcutaneous Fat of Abdomen Normal Range)',\n",
       " '187. Upper Limit (Subcutaneous Fat of Abdomen Normal Range)',\n",
       " '188. Visceral Fat',\n",
       " '189. Lower Limit (Visceral Fat of Abdomen Normal Range)',\n",
       " '190. Upper Limit (Visceral Fat of Abdomen Normal Range)',\n",
       " '191. Lower Limit (BFM of Arm & Leg Normal Range)',\n",
       " '192. Upper Limit (BFM of Arm & Leg Normal Range)',\n",
       " '193. Lower Limit (BFM of Trunk Normal Range)',\n",
       " '194. Upper Limit (BFM of Trunk Normal Range)',\n",
       " '195. Abdominal Fat',\n",
       " '196. V/S Ratio(Visceral Fat Area/Subcutaneous Fat Area ratio)',\n",
       " '197. SFA(Subcutaneous Fat Area)',\n",
       " '198. WHtR(Waist-Height Ratio)',\n",
       " '199. BAI(Body Adiposity Index)',\n",
       " '200. ABSI(A Body Shaped Index)',\n",
       " '201. Conicity Index',\n",
       " '202. LVR(Lean Mass-Visceral Fat Area Ratio)',\n",
       " '203. Upper Limit (WHtR Normal Range)',\n",
       " '204. Upper Limit (BAI Normal Range)',\n",
       " '205. Upper Limit (ABSI Normal Range)',\n",
       " '206. Upper Limit (Conicity Index Normal Range)',\n",
       " '207. Upper Limit (LVR Normal Range)',\n",
       " '208. 50khz-Ab Impedance',\n",
       " '209. 250khz-Ab Impedance',\n",
       " '210. WC(Waist Circumference)\\xa0',\n",
       " '211. Angle-R',\n",
       " '212. Angle_L',\n",
       " '213. Lower Limit (Abdominal Fat Normal Range)',\n",
       " '214. Upper Limit (Abdominal Fat Normal Range)',\n",
       " '215. Lower Limit (FFM Normal Range)',\n",
       " '216. Upper Limit (FFM Normal Range)',\n",
       " '217. SMM/WT',\n",
       " '218. Systolic2',\n",
       " '219. Diastolic2',\n",
       " '220. Pulse2',\n",
       " '221. Mean Artery Pressure2',\n",
       " '222. Pulse Pressure2',\n",
       " '223. Rate Pressure Product2',\n",
       " '224. BFM% of Whole Body',\n",
       " '225. ECM/BCM',\n",
       " '226. TBW/WT',\n",
       " '227. Lean Mass(%) of Right Arm (Ideal Weight)',\n",
       " '228. Lean Mass(%) of Left Arm (Ideal Weight)',\n",
       " '229. Lean Mass(%) of Trunk (Ideal Weight)',\n",
       " '230. Lean Mass(%) of Right Leg (Ideal Weight)',\n",
       " '231. Lean Mass(%) of Left Leg (Ideal Weight)',\n",
       " '232. Impedance Check',\n",
       " '233. BCM_T score',\n",
       " '234. BCM_Z score',\n",
       " '235. FFMI_T score',\n",
       " '236. FFMI_Z score',\n",
       " '237. FMI_T score',\n",
       " '238. FMI_Z score',\n",
       " '239. PBF_T score',\n",
       " '240. PBF_Z score',\n",
       " '241. Weight_T score',\n",
       " '242. Weight_Z score',\n",
       " '243. 50kHz-Whole Body Phase Angle_T score',\n",
       " '244. 50kHz-Whole Body Phase Angle_Z score',\n",
       " '245. TBW/WT_T Score',\n",
       " '246. TBW/WT_Z Score',\n",
       " '247. SMI(SMM/Wt)_T score',\n",
       " '248. SMI(SMM/Wt)_Z score',\n",
       " '249. ECM/BCM_T Score',\n",
       " '250. ECM/BCM Z Score',\n",
       " 'Source']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"mstr_ib970_tst\"\n",
    "import pandas as pd\n",
    "\n",
    "df_ib97_raw = pd.read_excel(\"mstr_ib970_tst.xlsm\")\n",
    "\n",
    "# verify \n",
    "list(df_ib97_raw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29c64d2-674a-4ddf-a857-a0c15ba0da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ib97_raw_flt = filter_by_value(df_ib97_raw, '2. ID', \"091725-1\")\n",
    "df_ib97_raw_flt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ffd28c-5317-49a2-89a0-2de1697fd824",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ib97_raw_flt = filter_by_value(df_ib97_raw, '2. ID', \"091725-1\")\n",
    "df_ib97_raw_flt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa96eb7f-d8f2-490c-a52a-decd9e73eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ib97_unique = drop_duplicate_source_files(df_ib97_raw_flt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2323b4c-5d82-4373-a7e0-f5f065c23a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ib97_raw_flt[['1. Name', '2. ID','source_file','13. Test Date / Time']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d412ee46-23f1-47fe-a6e0-27cc87bfc1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify list(df_ib97_raw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a15843f-2627-4e2d-b6f4-32c04704eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify df_ib97_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7978c848-eaf1-4e04-86c6-db9cc09dbcea",
   "metadata": {},
   "source": [
    "### This segment strips off the col names of numbers and produces \"df_m_ib97_nn\" and demonstrates slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f195eff-5777-47fd-a97b-6cf913663a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ib97_nn = strip_numbers_from_columns(df_ib97_raw)\n",
    "#verify print(list(df_ib97_nn.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206a3e44-8616-4091-bfd1-050c707a94da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# verify slicing df_ib97_nn[['ID','Test Date / Time','ECW/TBW']]  \n",
    "#,\"SMM (Skeletal Muscle Mass)\" , \"Weight\" , \"BMR (Basal Metabolic Rate)\" , \"ECW/TBW\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc877d-3975-4f99-8ad0-2201a03a60bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_duplicate_columns(df_ib97_nn, keep='first', log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92f6ee7-a940-4da0-a031-bc48a8029748",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_ib_duplicates_Test_Date_Time(df_ib97_nn, subset_cols=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d3df5-9764-4d2d-9f43-aa0c0f535045",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ib97_nn[\"Test Date / Time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96122701-c602-427a-b4af-8b2808eb14e4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "df_ib97_nn_s = df_ib97_nn.sort_values(by=\"Test Date / Time\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4be4d9-aa62-48a0-980b-4cccc51a1d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify df_ib97_nn_s[['ID','Test Date / Time','ECW/TBW']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89af3666-d670-4664-8372-5b89462c01a2",
   "metadata": {},
   "source": [
    "### This segment adds **media_cols** and fills them from data in the results **\"df_m_ib97_nn\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7d62a-dd49-48e0-9f58-80e287745925",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m_ib97_nn = prepend_empty_columns(df_ib97_nn, media_lst)\n",
    "# verify \n",
    "df_m_ib97_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe2c9ea-44f5-4c1b-aec9-fedfd32de66d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fill_ib_media_cols(df_mf_ib97_nn)\n",
    "df_mf_ib97_nn = fill_ib_media_cols(df_m_ib97_nn)\n",
    "# verify df_mf_ib97_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693b0804-34ef-47f5-a0ad-b2eef09d2076",
   "metadata": {},
   "source": [
    "### This segment eliminates \"COL\" duplicates in **df_mf_ib97_nn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5a7a7-a3de-46cf-a25d-e49eac750de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mf_ib97_nn = drop_duplicate_columns(df_mf_ib97_nn)\n",
    "print (\"df_mf_ib97_nn , No numbers, no duplicates OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59fc599-214d-4fc3-a973-6216616b8f5a",
   "metadata": {},
   "source": [
    "### This segment sorts \"df_mf_ib97_nn\" to get \"df_mf_ib97_nn_s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685dcb9c-b3e9-4406-86eb-6c669c7f29d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mf_ib97_nn_s = sort_by_timestamp(df_mf_ib97_nn )\n",
    "# verify df_mf_ib97_nn_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe3891-d5b6-4863-b596-4a9e04904324",
   "metadata": {},
   "source": [
    "### This segment eliminates \"ROW\" duplicates based on **\"test_time\" in **\"df_mf_ib97_nn\"** note: \"mf\" means media added and filled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db10d754-d056-4c7b-9176-2d9ea3a9dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mf_ib97_nn_s = df_mf_ib97_nn_s.drop_duplicates(subset=\"timestamp\", keep=\"first\")\n",
    "print(\"No timestamp duplicates **df_mf_ib97_nn** detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b49fc0-3fc3-4884-a5a8-6843e45b2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify list(df_mf_ib97_nn_s.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab4e8e2-cd92-4e40-ac96-6a6487f37946",
   "metadata": {},
   "source": [
    "# Set the \"plt_lst\" [the list ov cols to be plotted]\n",
    "1. **\"df_mf_ib97_nn_s\"** is the source data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbff9e7-007b-4459-9bb9-bc66627ab927",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_lst = list[\"ECW/TBW\", 'BMR (Basal Metabolic Rate)',\"SMM (Skeletal Muscle Mass)\",'VFA (Visceral Fat Area)']\n",
    "# verify \n",
    "plt_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2e20ca-29f2-47d3-b323-4b6dc244ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify df_mf_ib97_nn_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a92c6-539c-473c-b703-ef343e8118a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects the cols to be plotted \n",
    "plt_lst = [\"ECW/TBW\", 'BMR (Basal Metabolic Rate)',\"SMM (Skeletal Muscle Mass)\",'VFA (Visceral Fat Area)']\n",
    "df_mf_ib97_nn_s[[\"timestamp\",'ib_id']+plt_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e008e2cf-cad2-4174-9109-6f222844eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This filters out only the morning values of chosen data Col\n",
    "df_mf_ib97_nn_s_mrn = filter_by_value(df_mf_ib97_nn_s, 'ib_id', \"mrn\")\n",
    "# verify df_mf_ib97_nn_s_mrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f63432-f9c3-435a-9392-42d7471a5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_df_to_pickle(df_mf_ib97_nn_s, \"df_mf_ib97_nn_s_mrn.pkl\")\n",
    "print(\"df_mf_ib97_nn_s_mrn written to pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fc4cd7-7d2b-44d5-bd0c-b0c8644a6d62",
   "metadata": {},
   "source": [
    "# Normalize and subract 1 on \"df_mf_ib97_nn_s[plt_lst]\" col by col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f81492-20cb-43d6-98d8-9497b6a3c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mf_ib97_nn_s_n0 = df_mf_ib97_nn_s.copy() \n",
    "df_mf_ib97_nn_s_mrn_n0 = df_mf_ib97_nn_s_mrn.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ece8e64-0ff2-4e15-b558-913e161cbc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt_col = \"ECW/TBW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c983a748-f872-4c0b-b743-de4d437f2d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mf_ib97_nn_s_n0[plt_col]  =   scale_mean_to_one(df_mf_ib97_nn_s[plt_col])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe6854-e2be-4fa2-bc1b-8af3e0cf40c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_column(df_mf_ib97_nn_s_n0, plt_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecaa167-6ce3-41aa-af62-46586a010d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_lst = ['Weight',\"ECW/TBW\", 'BMR (Basal Metabolic Rate)',\"SMM (Skeletal Muscle Mass)\",'VFA (Visceral Fat Area)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f1731b-19eb-48b1-a5fb-3657e8f71cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for plt_col in plt_lst:\n",
    "    print(\"Now plotting:\", plt_col)\n",
    "    df_mf_ib97_nn_s_mrn_n0[plt_col]  =   scale_mean_to_one(df_mf_ib97_nn_s_mrn[plt_col])-1\n",
    "    plot_column(df_mf_ib97_nn_s_mrn_n0, plt_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5fa8ab-9f55-4fb7-a64f-722abb54a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_column(df_mf_ib97_nn_s_n0, \"ECW/TBW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3bdf6b-a514-4185-b61d-c16137e84b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41ad3678-a1c8-4cc5-94a9-ac00faa8623e",
   "metadata": {},
   "source": [
    "# WORKING cell \n",
    "1. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
