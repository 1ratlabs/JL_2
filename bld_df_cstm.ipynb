{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cbb72ef-b02f-48c2-a659-88d6c3fee5e2",
   "metadata": {},
   "source": [
    "# THE def function to **\"bld df_cstm\"** *the **\"df\"** used to perform a process on*  **spectic data**\n",
    "1. \"df_cust_nm\" = def bld_cstm(\"df_mn\", \"grp_nm\", \"dtv_rng_nm\", ib_id_nm, and \"srt_nm\"): are specified as arguments.\n",
    "2. Read \"df\" from XL [mstr_df_lst.xlxs] and slice **\"df_nm\"**\n",
    "3. Read \"grp_lst\" (list of rqrd cols) from XL [mstr_grp_lst.xlxs] and slice **\"grp_nm\"**\n",
    "4. Read \"dtv_rng\" (\"dtv_b,dtv_e) from XL [mstr_dtv_rng.xlxs] and slice **\"dtv_rng_nm\"**\n",
    "5. Read \"ib_id_nm\" from XL [mstr_ib_id_nm.xlxs] and slice **\"ib_id_nm_nm\"**\n",
    "6. Build \"df_grp\" by slicing \"df_nm\" with \"grp_lst[\"mstr_grp][\"grp_nm\"]\"\n",
    "7. Filter df_grp rows by col[\"dtv\"]\" from \"dtv_b\" to df_grp_e to get \"df_grp_rng\"\n",
    "8. Filter \"df_grp_rng\" rows by col[\"ib_id\"]\n",
    "9. Write \"df_grp_rng\" to pickle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98299ccf-3813-4908-b2cb-110b5d26dd77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Set up Global imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc44cd5a-40d5-4bff-8dde-de21b6877ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bhuns/miniconda3/bin/python\n",
      "note: THIS IS THE DIRECTORY PYTHON IS WORKING IN.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(\"note: THIS IS THE DIRECTORY PYTHON IS WORKING IN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1c6941-4bbc-421d-bcf4-ee3bb2055a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports required for Loading, sorting .csx files to create specific data sets ie mrn inbody readings. \n",
    "%run ./sys_funcs.py              # loads all the def functions in sys_funcs.py into memory\n",
    "#import sys_funcs                 # gives access to these def function digitalform that are in memory\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from datetime import time\n",
    "from sys_funcs import read_csv_to_array\n",
    "from sys_funcs import clean_wsl_path\n",
    "from sys_funcs import array_to_dt_row_dict\n",
    "from sys_funcs import make_blnk_update_row_dict\n",
    "from sys_funcs import transpose_csv_to_col_dict\n",
    "#from sys_funcs import update_values_with_config, get_update_result\n",
    "from sys_funcs import transfer_updates\n",
    "from sys_funcs import get_dtv_range\n",
    "from sys_funcs import universal_import\n",
    "from sys_funcs import parse_inbody_timestamp\n",
    "from sys_funcs import build_lut\n",
    "from sys_funcs import extract_a_column_as_df\n",
    "from sys_funcs import extract_multicolumns_as_df\n",
    "from sys_funcs import validate_and_sort_timestamps\n",
    "from sys_funcs import extract_and_filter_by_time_window\n",
    "from sys_funcs import read_file_dual_path\n",
    "from sys_funcs import write_file_dual_path\n",
    "from sys_funcs import asc_to_csv_cnv\n",
    "from collections.abc import Mapping\n",
    "import re\n",
    "#from sys_funcs import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40073c29-4be1-4d3f-86a4-f44d70c3d61f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print set to 1000 rows max\n"
     ]
    }
   ],
   "source": [
    "# set print rows  This worksheet sets maximum # of rows printed\n",
    "pd.set_option('display.max_rows', 1000)  # Adjust the number of rows to display\n",
    "# pd.reset_option('display.max_rows')  \n",
    "print('print set to 1000 rows max' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c780f0dd-0f8b-4742-9c02-a5a9512f19bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: timestamp = Test Date / Time does not work  use computed time stamp\n"
     ]
    }
   ],
   "source": [
    "print(\"NOTE: timestamp = Test Date / Time does not work  use computed time stamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acfc2e0-7d7c-44d0-8b42-b7b653431032",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Global def functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ad5ac93-e82e-450b-9b97-4b49dc414104",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "media_lst = [\n",
    "    \"timestamp\",\n",
    "    \"dtv\",\n",
    "    \"ib_id\",\n",
    "    \"cls\",\n",
    "    \"cmmnts\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "152dd983-4fa2-4fc8-a108-3bc04d9dea4a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3rd version def drop_duplicates_by_test_time(df, keep='first', log=True):\n",
    "def drop_duplicates_by_test_time(df, keep='first', log=True):\n",
    "    \"\"\"\n",
    "    Removes duplicate rows based on the 'Test Date / Time' column.\n",
    "    Keeps only the first (or last) occurrence.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Identify duplicate timestamps (beyond the one we keep)\n",
    "    dupes = (\n",
    "        df.loc[df.duplicated(subset=['Test Date / Time'], keep=keep), 'Test Date / Time']\n",
    "        .astype(str)\n",
    "        .values\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    # Drop duplicate rows\n",
    "    df = df.drop_duplicates(subset=['Test Date / Time'], keep=keep)\n",
    "\n",
    "    # Optional logging\n",
    "    if log and dupes:\n",
    "        print(\"Removed duplicate rows for timestamps:\", dupes)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "952b0a1e-5cd1-4795-9378-6cc489a18932",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# RAW old def drop_duplicates_by_test_time(df, keep='first', log=True):\n",
    "\n",
    "def drop_duplicates_by_test_time(df, keep='first', log=True):\n",
    "    \"\"\"\n",
    "    Removes duplicate rows based on the 'Test Date / Time' column.\n",
    "    Keeps only the first (or last) occurrence of each timestamp.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Input DataFrame.\n",
    "    keep : {'first', 'last'}, default 'first'\n",
    "        Which duplicate to keep.\n",
    "    log : bool, default True\n",
    "        Whether to print which timestamps were removed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with duplicates removed.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Identify duplicate timestamps (beyond the one we keep)\n",
    "    dupes = df.loc[df.duplicated(subset=['Test Date / Time'], keep=keep),\n",
    "                   'Test Date / Time'].tolist()\n",
    "\n",
    "    # Drop duplicate rows\n",
    "    df = df.drop_duplicates(subset=['Test Date / Time'], keep=keep)\n",
    "\n",
    "    # Optional logging\n",
    "    if log and dupes:\n",
    "        print(\"Removed duplicate rows for timestamps:\", dupes)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e73aa88c-6a17-41b0-ab8d-265ea995135d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def strip_numbers_from_columns(df):\n",
    "import re\n",
    "\n",
    "def strip_numbers_from_columns(df):\n",
    "    \"\"\"\n",
    "    Removes leading/trailing numbers and any leftover separators\n",
    "    so that cases like '1.0ID' become 'ID'.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    new_cols = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        cleaned = col\n",
    "\n",
    "        # Remove leading numbers + separators\n",
    "        cleaned = re.sub(r'^\\d+[\\s\\-\\_\\.:]*', '', cleaned)\n",
    "\n",
    "        # Remove trailing numbers + separators\n",
    "        cleaned = re.sub(r'[\\s\\-\\_\\.:]*\\d+$', '', cleaned)\n",
    "\n",
    "        # Remove leftover leading/trailing punctuation (.,-_:) after number removal\n",
    "        cleaned = re.sub(r'^[\\.\\-\\_\\:]+', '', cleaned)\n",
    "        cleaned = re.sub(r'[\\.\\-\\_\\:]+$', '', cleaned)\n",
    "\n",
    "        new_cols[col] = cleaned\n",
    "\n",
    "    return df.rename(columns=new_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eddfc987-92f4-4957-a631-a335c66df082",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Use drop duplicate function If duplicates are found\n",
    "def drop_duplicate_columns(df, keep='first', log=True):\n",
    "    \"\"\"\n",
    "    Removes duplicate column names from a DataFrame, keeping only the first\n",
    "    (or last) occurrence. Useful after column-cleaning steps that may cause\n",
    "    collisions. good i'm moving this around because I want to go ahead and do the I'm talking too\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Input DataFrame.\n",
    "    keep : {'first', 'last'}, default 'first'\n",
    "        Which duplicate to keep.\n",
    "    log : bool, default True\n",
    "        Whether to print which columns were removed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with duplicate columns removed.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Identify duplicates beyond the one we keep\n",
    "    dupes = df.columns[df.columns.duplicated(keep=keep)].tolist()\n",
    "\n",
    "    # Drop them\n",
    "    df = df.loc[:, ~df.columns.duplicated(keep=keep)]\n",
    "\n",
    "    # Optional logging\n",
    "    if log and dupes:\n",
    "        print(\"Removed duplicate columns:\", dupes)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2a70e40-66fb-4e3a-a45e-2bb221d52eea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def prepend_empty_columns(df, col_list):\n",
    "def prepend_empty_columns(df, col_list):\n",
    "    \"\"\"\n",
    "    Prepend empty columns (from col_list) to the front of df.\n",
    "    Returns a new DataFrame.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    # Create empty columns with same row count\n",
    "    empty_df = pd.DataFrame(\n",
    "        {col: [None] * len(df) for col in col_list}\n",
    "    )\n",
    "\n",
    "    # Prepend them\n",
    "    return pd.concat([empty_df, df], axis=1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "275e0f36-1c1c-4582-9144-f6289464ed57",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# old raw def fill_ib_media_cols(df):\n",
    "#filling the media cols\n",
    "def fill_ib_media_cols(df):\n",
    "    \"\"\"\n",
    "    Fills the 5 leading operator columns for InBody datasets:\n",
    "      - timestamp  ← parsed from 'Test Date / Time' (YYYYMMDDHHMMSS)\n",
    "      - dtv        ← days since 1900‑01‑01\n",
    "      - ib_id      ← 'mrn' if test time 03:00–23:59, else 'eve'\n",
    "      - cls        ← NaN\n",
    "      - cmmnts     ← NaN\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- 1. timestamp (correct parsing) ------------------------\n",
    "    df['timestamp'] = pd.to_datetime(\n",
    "        df['Test Date / Time'].astype(str),\n",
    "        format=\"%Y%m%d%H%M%S\",\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # --- 2. dtv: days since 1900‑01‑01 -------------------------\n",
    "    origin = pd.Timestamp(\"1900-01-01\")\n",
    "    df['dtv'] = (df['timestamp'] - origin).dt.days\n",
    "\n",
    "    # --- 3. ib_id classification -------------------------------\n",
    "    def classify_ib_id(ts):\n",
    "        if pd.isna(ts):\n",
    "            return np.nan\n",
    "        hour = ts.hour\n",
    "        return \"mrn\" if 3 <= hour <= 12 else \"eve\"\n",
    "\n",
    "    df['ib_id'] = df['timestamp'].apply(classify_ib_id)\n",
    "\n",
    "    # --- 4. cls ------------------------------------------------\n",
    "    df['cls'] = np.nan\n",
    "\n",
    "    # --- 5. cmmnts ---------------------------------------------\n",
    "    df['cmmnts'] = np.nan\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54c8b4e1-7019-4f5e-8bc4-a2dbeabeb08b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# revised def fill_ib_media_cols(df):\n",
    "def fill_ib_media_cols(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- 1. Clean and parse timestamp --------------------------\n",
    "    def fix_ts(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        # Convert float → int safely\n",
    "        try:\n",
    "            x_int = int(float(x))\n",
    "        except:\n",
    "            return np.nan\n",
    "        # Zero‑pad to 14 digits (YYYYMMDDHHMMSS)\n",
    "        s = str(x_int).zfill(14)\n",
    "        return pd.to_datetime(s, format=\"%Y%m%d%H%M%S\", errors=\"coerce\")\n",
    "\n",
    "    df['timestamp'] = df['Test Date / Time'].apply(fix_ts)\n",
    "\n",
    "    # --- 2. dtv ------------------------------------------------\n",
    "    origin = pd.Timestamp(\"1900-01-01\")\n",
    "    df['dtv'] = (df['timestamp'] - origin).dt.days\n",
    "\n",
    "    # --- 3. ib_id ----------------------------------------------\n",
    "    def classify_ib_id(ts):\n",
    "        if pd.isna(ts):\n",
    "            return np.nan\n",
    "        hour = ts.hour\n",
    "        return \"mrn\" if 3 <= hour <= 12 else \"eve\"\n",
    "\n",
    "    df['ib_id'] = df['timestamp'].apply(classify_ib_id)\n",
    "\n",
    "    # --- 4–5. cls, cmmnts --------------------------------------\n",
    "    df['cls'] = np.nan\n",
    "    df['cmmnts'] = np.nan\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13c4627e-8ec3-4306-8d0c-2af47e7975d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the rows by timestamp\n",
    "def sort_by_timestamp(df):\n",
    "    \"\"\"\n",
    "    Sorts an InBody dataframe by the 'timestamp' column\n",
    "    in ascending chronological order.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(by='timestamp', ascending=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a9b43c2-dace-4c30-beeb-97667a3f6099",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# A function to combine frames of IB77 with IB97 and sorting into one data frame. N\n",
    "# this will be used on a column by column basis for a list of columns.\n",
    "def combine_weight_frames(df_a, df_b, ts_col=\"timestamp\", wt_a=\"2. wt\", wt_b=\"4. wt\"):\n",
    "    \"\"\"\n",
    "    Combines two dataframes with different weight column names into a single\n",
    "    dataframe with columns: timestamp, wt, sorted by timestamp.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_a : pd.DataFrame\n",
    "        First dataframe containing a timestamp column and a weight column.\n",
    "    df_b : pd.DataFrame\n",
    "        Second dataframe containing a timestamp column and a weight column.\n",
    "    ts_col : str, optional\n",
    "        Name of the timestamp column (default 'timestamp').\n",
    "    wt_a : str, optional\n",
    "        Weight column name in df_a (default '2. wt').\n",
    "    wt_b : str, optional\n",
    "        Weight column name in df_b (default '4. wt').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Combined dataframe with columns: timestamp, wt, sorted by timestamp.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize df_a\n",
    "    df_a_norm = df_a[[ts_col, wt_a]].rename(columns={wt_a: \"wt\"})\n",
    "\n",
    "    # Normalize df_b\n",
    "    df_b_norm = df_b[[ts_col, wt_b]].rename(columns={wt_b: \"wt\"})\n",
    "\n",
    "    # Stack them vertically\n",
    "    df_combined = pd.concat([df_a_norm, df_b_norm], ignore_index=True)\n",
    "\n",
    "    # Sort by timestamp\n",
    "    df_combined = df_combined.sort_values(by=ts_col).reset_index(drop=True)\n",
    "\n",
    "    return df_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7eab1d0a-6cac-4215-91cc-b6f15049d11b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# remove duplicates on the basis of timestamp\n",
    "def remove_ib_duplicates(df, subset_cols=None):\n",
    "    \"\"\"\n",
    "    Removes duplicate InBody rows based on key identifying columns.\n",
    "    Default behavior: remove duplicates based on ['ID', 'timestamp'].\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Default duplicate definition\n",
    "    if subset_cols is None:\n",
    "        subset_cols = ['timestamp']\n",
    "        # subset_cols = ['ID', 'timestamp']\n",
    "    # Remove duplicates, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset=subset_cols, keep='first')\n",
    "\n",
    "    # Reset index for cleanliness\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6f8def7-1c3d-4df6-b53a-0d3ea6978f0e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# veirfy if rows exixt in master_timestamps(df_master, df_new, ts_col=\"timestamp\"):\n",
    "def filter_new_rows_by_master_timestamps(df_master, df_new, ts_col=\"timestamp\"):\n",
    "    \"\"\"\n",
    "    Filters df_new so that only rows whose timestamps appear in df_master remain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_master : pd.DataFrame\n",
    "        The master dataframe containing valid timestamps.\n",
    "    df_new : pd.DataFrame\n",
    "        The new dataframe to be filtered.\n",
    "    ts_col : str, optional\n",
    "        The name of the timestamp column (default is 'timestamp').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A filtered version of df_new containing only rows whose timestamps\n",
    "        exist in df_master.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the set of valid timestamps from the master dataframe\n",
    "    valid_timestamps = set(df_master[ts_col])\n",
    "\n",
    "    # Filter df_new to keep only rows with timestamps in the master set\n",
    "    df_filtered = df_new[df_new[ts_col].isin(valid_timestamps)].copy()\n",
    "\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25e2d5e7-641f-4c83-ae6a-33ea309cb874",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def keep_only_new_timestamps(df_master, df_new, ts_col=\"timestamp\")\n",
    "def keep_only_new_timestamps(df_master, df_new, ts_col=\"timestamp\"):\n",
    "    \"\"\"\n",
    "    Returns only the rows in df_new whose timestamps do NOT exist in df_master.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_master : pd.DataFrame\n",
    "        The master dataframe containing timestamps already ingested.\n",
    "    df_new : pd.DataFrame\n",
    "        The new dataframe to be filtered.\n",
    "    ts_col : str, optional\n",
    "        The name of the timestamp column (default is 'timestamp').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A filtered version of df_new containing only rows with timestamps\n",
    "        NOT present in df_master.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the set of timestamps already in the master\n",
    "    existing_ts = set(df_master[ts_col])\n",
    "\n",
    "    # Keep only rows whose timestamp is NOT in the master\n",
    "    df_filtered = df_new[~df_new[ts_col].isin(existing_ts)].copy()\n",
    "\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb376852-d4e8-4f86-813a-3b09388513fd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def append_rows_with_master_schema(master_df, adder_df):\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def append_rows_with_master_schema(master_df, adder_df):\n",
    "    \"\"\"\n",
    "    Appends rows from adder_df into master_df while enforcing the master_df schema.\n",
    "\n",
    "    For each row in adder_df:\n",
    "      - Columns that exist in adder_df are copied.\n",
    "      - Columns missing from adder_df are filled with NaN.\n",
    "      - All master_df columns are preserved in order.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    master_df : pd.DataFrame\n",
    "        The master dataframe with the full schema.\n",
    "    adder_df : pd.DataFrame\n",
    "        The dataframe containing rows to append (subset of master columns).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Updated master_df with new rows appended.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reindex adder_df to match master_df columns, filling missing columns with NaN\n",
    "    adder_aligned = adder_df.reindex(columns=master_df.columns)\n",
    "\n",
    "    # Append and return\n",
    "    return pd.concat([master_df, adder_aligned], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "052ecc49-53e1-431c-8cd9-be72ca600e80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def filter_by_value(df, column, value):\n",
    "def filter_by_value(df, column, value):\n",
    "    \"\"\"\n",
    "    Returns a filtered DataFrame containing only rows where df[column] == value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The dataframe to filter.\n",
    "    column : str\n",
    "        The column name to filter on.\n",
    "    value : any\n",
    "        The value that the column must match.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A filtered dataframe containing only matching rows.\n",
    "    \"\"\"\n",
    "    return df[df[column] == value].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "399b4af2-bb01-4c5c-bbc8-b10d524bafd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_df_to_pickle(df, filename):\n",
    "def write_df_to_pickle(df, filename):\n",
    "    \"\"\"\n",
    "    Writes a DataFrame to a pickle file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The dataframe to save.\n",
    "    filename : str\n",
    "        The pickle filename, e.g. 'mydata.pkl'.\n",
    "    \"\"\"\n",
    "    df.to_pickle(filename)\n",
    "\n",
    "# usage \n",
    "# write_df_to_pickle(df, \"df.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31a7611d-333d-4bd8-931b-4251d5eefada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_df_from_pickle(filename):\n",
    "\n",
    "def load_df_from_pickle(filename):\n",
    "    \"\"\"\n",
    "    Loads a DataFrame from a pickle file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Path to the pickle file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "    \"\"\"\n",
    "    return pd.read_pickle(filename)\n",
    "\n",
    "    # usage \n",
    "    # df = load_df_from_pickle(\"df.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f65cee1d-c271-4489-b29a-daaeb5fa5811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scale_mean_to_one(series):\n",
    "def scale_mean_to_one(series):\n",
    "    \"\"\"Scale a Pandas Series so that its mean becomes 1.\"\"\"\n",
    "    mean_val = series.mean()\n",
    "    return series / mean_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc4a8e1e-54bd-4261-ac7e-efca87d980c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_column(df, col_name):\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_column(df, col_name):\n",
    "    \"\"\"\n",
    "    Plot a single column from a dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing the column.\n",
    "    col_name : str\n",
    "        The name of the column to plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(df[col_name], marker='o', linestyle='-', linewidth=1)\n",
    "    plt.title(f\"{col_name} over index\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(col_name)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e98e1f-6752-431e-a171-8d8fa80f3f04",
   "metadata": {},
   "source": [
    "# START +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09485323-85e1-4315-bb9a-32c5d12c0e5a",
   "metadata": {},
   "source": [
    "## Read \"prd\" (\"dtv_strt\",dtv_end,\"ib_id\") from XL [prd_lst.xlxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4659101e-661f-40af-be64-affeec7470eb",
   "metadata": {},
   "source": [
    "## Read \"grp\" (list of rqrd cols) from XL [grp_lst.xlxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e1c0af-775b-4d1b-b15f-65a504ff8152",
   "metadata": {},
   "source": [
    "## Import **df_ib** from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49ed9d67-864f-4473-9bb9-a4c035d69ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = load_df_from_pickle(\"df.pkl\")\n",
    "df_ib = load_df_from_pickle(\"df_ib.pkl\")\n",
    "# verify df_ib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca818d29-4286-4ed8-9f65-dcee2cce6e40",
   "metadata": {},
   "source": [
    "## Build \"df_cstm_grp\" by slicing \"df\" with \"grp_lst[\"grp\"]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d469942d-922b-4097-90bf-21e928d56612",
   "metadata": {},
   "source": [
    "## Import **df_ib** from pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35be812-4267-4962-a912-386beed9d696",
   "metadata": {},
   "source": [
    "## Import **df_ib** from pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469f0eb3-33df-449e-8bc2-d157fe2c0dd5",
   "metadata": {},
   "source": [
    "## Import **df_ib** from pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938a244-cbdd-42eb-bb3d-4a099a710deb",
   "metadata": {},
   "source": [
    "## Import **df_ib** from pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f47dbf-4023-44ef-9e54-122f4e8b4a23",
   "metadata": {},
   "source": [
    "## Import **df_ib** from pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46678db1-ce06-42ff-9621-028c868d1588",
   "metadata": {},
   "source": [
    "## Import **df_ib** from pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ad3678-a1c8-4cc5-94a9-ac00faa8623e",
   "metadata": {},
   "source": [
    "# WORKING cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2356bdf2-4235-4383-84a6-6e2e13a73b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the list of columns you want to include in the analysis and plotting\n",
    "grp = [\"ECW/TBW\",\n",
    "           \"BMR (Basal Metabolic Rate)\",\n",
    "           \"SMM (Skeletal Muscle Mass)\",\n",
    "           \"VFA (Visceral Fat Area)\"]\n",
    "# verify grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e07e033c-0338-4241-a692-d094ed7f9bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df for meda plus grp values\n",
    "df_cstm = df_ib[[\"timestamp\",\"dtv\"] + grp]\n",
    "# verify df_cstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3bdf6b-a514-4185-b61d-c16137e84b51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
