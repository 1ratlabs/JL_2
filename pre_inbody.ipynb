{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a52f2da4-ba8f-4ee0-981e-53ad60a73aa7",
   "metadata": {},
   "source": [
    "# Procedure when data added ; Start from \"pre_inbody.\" [HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e174ab8-e3d9-42bd-ba0f-f914aea2938c",
   "metadata": {},
   "source": [
    "## Can use \"[RESET/RESTART]\" because recalculated data is stored in pickle\n",
    "1. Move new data from \"C:\\Users\\bhuns\\OneDrive\\___Health Data\\__DD studies\\InBody CSV\\ib97\" into \"JL_1/data/ib97\"\n",
    "2. GO to \"pre_inbody\"  and Hit >> [RESET/RESTART] to erase old results, reset kernal and run the cell. [Results in pickle]\n",
    "3. GO to \"nrmlz_data_dict\"  and Hit >> [RESET/RESTART] to erase old results, reset kernal and run the cell. [Results in pickle]\n",
    "4. GO to \"plot_sandbox\"  and Hit >> [RESET/RESTART] to erase old results, reset kernal and run the cell. [Results in graphs]\n",
    "5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc44cd5a-40d5-4bff-8dde-de21b6877ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bhuns/miniconda3/bin/python\n",
      "note: THIS IS THE DIRECTORY PYTHON IS WORKING IN.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(\"note: THIS IS THE DIRECTORY PYTHON IS WORKING IN.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb72ef-b02f-48c2-a659-88d6c3fee5e2",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e1c6941-4bbc-421d-bcf4-ee3bb2055a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports required for Loading, sorting .csx files to create specific data sets ie mrn inbody readings. \n",
    "%run ./sys_funcs.py              # loads all the def functions in sys_funcs.py into memory\n",
    "#import sys_funcs                 # gives access to these def function digitalform that are in memory\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from datetime import time\n",
    "from sys_funcs import read_csv_to_array\n",
    "from sys_funcs import clean_wsl_path\n",
    "from sys_funcs import array_to_dt_row_dict\n",
    "from sys_funcs import make_blnk_update_row_dict\n",
    "from sys_funcs import transpose_csv_to_col_dict\n",
    "#from sys_funcs import update_values_with_config, get_update_result\n",
    "from sys_funcs import transfer_updates\n",
    "from sys_funcs import get_dtv_range\n",
    "from sys_funcs import universal_import\n",
    "from sys_funcs import parse_inbody_timestamp\n",
    "from sys_funcs import build_lut\n",
    "from sys_funcs import extract_a_column_as_df\n",
    "from sys_funcs import extract_multicolumns_as_df\n",
    "from sys_funcs import validate_and_sort_timestamps\n",
    "from sys_funcs import extract_and_filter_by_time_window\n",
    "from sys_funcs import read_file_dual_path\n",
    "from sys_funcs import write_file_dual_path\n",
    "from sys_funcs import asc_to_csv_cnv\n",
    "#from sys_funcs import "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962d6002-aa53-4f83-b96e-abedf1fceb86",
   "metadata": {},
   "source": [
    "# DEF functions for this workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b9008e8-0977-4a44-9b04-ff8b2aec28f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name the list in the call and this function will:\n",
    "# ask for a substring and make a list of names that contain it.\n",
    "def filter_column_names_interactive(df_col_nms):\n",
    "    \"\"\"\n",
    "    Prompts for a substring and prints matching column names from df_col_nms.\n",
    "    Assumes df_col_nms is a single-column DataFrame of column names.\n",
    "    \"\"\"\n",
    "    substring = input(\"Enter substring to filter column names: \").strip()\n",
    "    matches = df_col_nms[df_col_nms.iloc[:, 0].str.contains(substring, case=False, na=False)]\n",
    "    \n",
    "    if matches.empty:\n",
    "        print(f\"No column names contain '{substring}'.\")\n",
    "    else:\n",
    "        print(f\"Column names containing '{substring}':\")\n",
    "        for name in matches.iloc[:, 0]:\n",
    "            print(f\"  - {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae6d4248-736b-4b6e-a986-cb47221b9f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this def function takes raw imported a data_dict cleans up the '14. Test Date / Time' then` This 13 for the 770\n",
    "# attachs a \"Cleaned_Timestamp\" and a \"dtv\" \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "503c0610-0370-4e2b-a6c0-f01b041997ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_dict(data_dict):\n",
    "    cleaned_dict = data_dict.copy()\n",
    "    base_date = datetime(1900, 1, 1)\n",
    "\n",
    "    def safe_parse(ts):\n",
    "        try:\n",
    "            return (datetime.strptime(ts, \"%Y%m%d%H%M%S\") - base_date).days\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse timestamp {ts}: {e}\")\n",
    "            return np.nan\n",
    "\n",
    "    timestamps = data_dict.get(\"13. Test Date / Time\", [])\n",
    "    cleaned_dict[\"Cleaned_Timestamp\"] = [\n",
    "        str(ts).strip()[:14] if str(ts).strip().lower() != \"nan\" and len(str(ts).strip()) >= 14 else np.nan\n",
    "        for ts in timestamps\n",
    "    ]\n",
    "    cleaned_dict[\"dtv\"] = [\n",
    "        safe_parse(ts) if isinstance(ts, str) and len(ts) == 14 else np.nan\n",
    "        for ts in cleaned_dict[\"Cleaned_Timestamp\"]\n",
    "    ]\n",
    "    return cleaned_dict\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df2a55fe-a4e0-493e-b31e-0806b8838121",
   "metadata": {},
   "source": [
    "Replaced by above\n",
    "def clean_data_dict(data_dict):\n",
    "    \"\"\"\n",
    "    Cleans the input data_dict by:\n",
    "    - Stripping and truncating '14. Test Date / Time' to 14 characters\n",
    "    - Creating a 'Cleaned_Timestamp' column\n",
    "    - Creating a 'dtv' column: days since 1/1/1900\n",
    "    Returns a cleaned copy of the input dictionary.\n",
    "    \"\"\"\n",
    "    cleaned_dict = data_dict.copy()\n",
    "\n",
    "    # Step 1: Clean and truncate timestamp strings\n",
    "    cleaned_dict[\"Cleaned_Timestamp\"] = [\n",
    "        str(ts).strip()[:14] if str(ts).strip().lower() != \"nan\" and len(str(ts).strip()) >= 14 else np.nan\n",
    "        for tststststs in cleaned_dict.get(\"14. Test Date / Time\", [])\n",
    "    ]\n",
    "\n",
    "    # Step 2: Convert to datetime and calculate days since 1/1/1900\n",
    "    base_date = datetime(1900, 1, 1)\n",
    "    def safe_parse(ts):\n",
    "        try:\n",
    "            return (datetime.strptime(ts, \"%Y%m%d%H%M%S\") - base_date).days\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    cleaned_dict[\"dtv\"] = [\n",
    "        safe_parse(ts) if isinstance(ts, str) and len(ts) == 14 else np.nan\n",
    "        for ts in cleaned_dict[\"Cleaned_Timestamp\"]\n",
    "    ]\n",
    "\n",
    "    return cleaned_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e277b49-0cd8-4fa9-95e9-e2e93c74ec70",
   "metadata": {},
   "source": [
    "# Create **ib970** by Loading **970.csv** files"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b75fb136-0159-498d-baae-0a3ce14c358f",
   "metadata": {},
   "source": [
    "# RAW for saftey gets only 1\n",
    "# Loads all the files from ib970 file  >>> folder_path=\"/home/bhuns/JL_1/data/ib97\",\n",
    "# those are copied in from \"C:\\Users\\bhuns\\OneDrive\\___Health Data\\__DD studies\\InBody CSV\\ib97\"\n",
    "# uses \"from sys_funcs import universal_import\"\n",
    "ib970 = universal_import(\n",
    "    folder_path=\"/home/bhuns/JL_2/data/ib97\",\n",
    "    pattern=\"091725*\",\n",
    "    #expected_columns=250,\n",
    "    df_name=\"ib970\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "698aa149-cc56-4823-b2f2-1d96a7fcf75b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2419682144.py, line 11)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfolder_path=\"/home/bhuns/JL_2/data/ib97\", OK pardon\u001b[39m\n                                              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "print(\"091725\")\n",
    "ib970_a = universal_import(\n",
    "    folder_path=\"/home/bhuns/JL_2/data/ib97\",\n",
    "    pattern=\"091725*\",\n",
    "    df_name=\"ib970_a\"\n",
    ")\n",
    "print(\"091725\",ib970_a)\n",
    "print(ib970_a.columns)\n",
    "      \n",
    "ib970_b = universal_import(\n",
    "    folder_path=\"/home/bhuns/JL_2/data/ib97\", OK pardon\n",
    "    pattern=\"251201*\",\n",
    "    df_name=\"ib970_b\"\n",
    ")\n",
    "print(\"251201\",ib970_b)\n",
    "\n",
    "# Combine them (assuming they return DataFrames)\n",
    "import pandas as pd\n",
    "ib970 = pd.concat([ib970_a, ib970_b], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0236db-05bb-4aa5-92e3-90cc61dae733",
   "metadata": {},
   "source": [
    "# Create **ib970cln** *and* **df_ib970cln_col_nms** from **ib970**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9002f77-53ac-436c-b799-d32d5879cc7e",
   "metadata": {},
   "source": [
    "## Cleanup the **'13. Test Date / Time'** so **datetime** will work and add cols ie. keys:\n",
    "1. **Cleaned_Timestamp**\n",
    "2. **dtv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ae8d8d-adfe-46ac-9a55-024bcdb49cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Mapping\n",
    "import pandas as pd\n",
    "\n",
    "def kind_of(var):\n",
    "    if isinstance(var, pd.DataFrame):\n",
    "        return \"dataframe\"\n",
    "    if isinstance(var, Mapping):   # covers dict, OrderedDict, defaultdict, etc.\n",
    "        return \"dict-like\"\n",
    "    return \"other\"\n",
    "\n",
    "# Usage\n",
    "\n",
    "print(kind_of(pd.DataFrame({\"ib970cln \":[1,2]})))  # dataframe\n",
    "print(kind_of({\"ib970cln \": 1}))                   # dict-like\n",
    "print(kind_of([1,2,3]))                            # other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad527cda-3bd3-4b13-bba3-0b1a96d29596",
   "metadata": {},
   "outputs": [],
   "source": [
    "kind_of(ib970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6240179f-5527-4808-87fd-664b2686aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify \n",
    "# \n",
    "ib970    # all rows but not in chronological order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b65042-c126-4f1a-8c9c-d91c77b3775f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c761c8-4f2f-4f58-99c7-e79a4f85ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this calls the def clean_data_dict () to clean bad testdate data and add \"dtv\" & \"Cleaned_Timestamp\" NANs still included\n",
    "ib970cln = clean_data_dict(ib970)\n",
    "print(\"ib970cln has been created with dtv & Cleaned_Timestamp cols added \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295671fd-3b26-461f-a6d3-d27266dd8eb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# verify still not in temporal order yet\n",
    "#\n",
    "ib970cln       #this has all the rows but not in chronological oreder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0437554-f8b7-4b80-a694-00dc5d7f5d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3cf8038-02ea-4aee-82da-987cdc8d8b68",
   "metadata": {},
   "source": [
    "### Read the **df_ib970cln_col_nms** *from* **ib970cln**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698061b5-91e5-42e4-ba3c-f6310cfe655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ib970cln_col_nms = pd.DataFrame(ib970cln.columns)\n",
    "print(\"df_ib970cln_col_nms has been created \")\n",
    "# df_ib970cln_col_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e73e77-7283-4879-bc80-fe00946491ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# verify\n",
    "# \n",
    "df_ib970cln_col_nms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a433a6c-7668-43dc-8ede-5b656853da6e",
   "metadata": {},
   "source": [
    "## Save **ib970cln** and **df_ib970cln_col_nms** to **pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55347a68-1224-4cff-add5-d27a2329acce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ib970cln to a file\n",
    "import pickle\n",
    "with open(\"ib970cln.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ib970cln, f)\n",
    "print(\" ib970cln is saved to pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ebb35-88a5-44c9-aeff-096043ac06fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify \n",
    "# \n",
    "ib970cln    # all rows but not in chronological order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e0f97d-7474-49b2-8a06-213f4fe6b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify \n",
    "# \n",
    "ib970cln_dtstmp = ib970cln[[\"dtv\", \"Cleaned_Timestamp\"]]    # time and datestamp df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4abaa-d892-40aa-b0a1-4873c404b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify \n",
    "# \n",
    "ib970cln_dtstmp    #[\"ib970cln_dtstmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c60a4f0-cfe5-4546-b57c-2a6aaa49fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df_ib970cln_col_nms to a file\n",
    "import pickle\n",
    "with open(\"df_ib970cln_col_nms.pkl\", \"wb\") as f:\n",
    "    pickle.dump( df_ib970cln_col_nms, f)\n",
    "print(\"  df_ib970cln_col_nms is saved to pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e2c02-a7a0-4d8f-b3b4-0772318b7040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ib970cln_dtstmp to a file\n",
    "import pickle\n",
    "with open(\"ib970cln_dtstmp.pkl\", \"wb\") as f:\n",
    "    pickle.dump( ib970cln_dtstmp, f)\n",
    "print(\"  ib970cln_dtstmp is saved to pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac55c389-deeb-4842-b6f6-cd3752e8b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "# ib970mrn_grp1_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd7ceee-3a6e-4cd8-98b9-bae4236600bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "# \n",
    "ib970cln_dtstmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211ab96a-1113-4eaa-a5be-aab466bacde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(ib970cln_dtstmp['Cleaned_Timestamp'].sort_values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f92646e-9fe0-4e16-bf3b-2b1c55f1b0ad",
   "metadata": {},
   "source": [
    "# Creating  **ib970mrn_grp_dict** from **df_ib970cln_col_nms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0879de-1ac3-40f8-b604-3b8ed484af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(ib970cln['Cleaned_Timestamp'].sort_values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51dcfc1-17cf-4630-965f-1ee3957e4e63",
   "metadata": {},
   "source": [
    "##  **ib970mrn_grp1** from **ib970cln**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d950f559-21db-41f4-a103-bd866d7bbe1a",
   "metadata": {},
   "source": [
    "### Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ddd6a-1484-49e9-9c6b-af00e6c8140e",
   "metadata": {},
   "source": [
    "#### \n",
    "1. used to specify the keys[data_cols] needed in **grp1**\n",
    "2. The are selected from **df_ib970cln_col_nms**\n",
    "3. They are chosen manually by copying the names from **df_ib970cln_col_nms list** and pasting them into **ib970mrn_grp1** specification\n",
    "4. To aid the a subscript from the input reduces the number of cos in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7182bcaa-073d-45d6-a7de-5dc0649293bb",
   "metadata": {},
   "source": [
    "#### 1. ib970mrn_grp all have keys= \"dtv\" and \"Cleaned_Timestamp\" + a different set of key combinations. [a key is the name of data col]\n",
    "2. The ib970mrn_grp#_dict {key: grp# .........}\n",
    "3. Each grp# starts with the \"dtv\" & \"Cleaned_Timestamp\" cols ready to have more cols added to the list.\n",
    "4. A filter that shows all the \"df_ib970cln_col_nms\" that contain str supplied by input functions.\n",
    "5. Chosen \"col names\" are copied into the list for that grp#\n",
    "6. The grp# is stored via pickel and is read from pick either to use or to edit.\n",
    "7. When editing, if a grp# does not exist the templet with the 2 cols is profide to star a new group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee4d4e-6fd1-4d12-b99f-c174e9dc8571",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00ef10bc-26e5-4477-b197-db86765dbefb",
   "metadata": {},
   "source": [
    "#### The following readings are not working\n",
    "184. Subcutaneous Fat\n",
    "188. Visceral Fat\n",
    "195. Abdominal Fat\n",
    "196. V/S Ratio(Visceral Fat Area/Subcutaneous Fat Area ratio)\n",
    "197. SFA(Subcutaneous Fat Area)\n",
    "208. 50khz-Ab Impedance\n",
    "209. 250khz-Ab Impedance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff29404-adb3-465c-92b8-eb80903d0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of data_col names that have subsript ie THIS IS AN AID TO FIND THE RIGHT DA_COLS\n",
    "df_col_nms = df_ib970cln_col_nms\n",
    "df_col_nms\n",
    "#filter_column_names_interactive(df_col_nms)\n",
    "# filter_column_names_interactive(df_col_nms)\n",
    "# link for ECM&BCM   https://copilot.microsoft.com/shares/J9Fbyou7dZXqDwmDD78S7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ab9fa4-2dbe-419a-a874-73f236d4518d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for verification\n",
    "# \n",
    "ib970mrn_grp1_nms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb894b7-cd3c-46f3-bff8-5fb6c71c63b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Creating  **ib970mrn_grp1** from **ib970cln**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc7ab59-4a8d-48bd-b9a8-c32368be7e47",
   "metadata": {},
   "source": [
    "###  Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a68b7d-6237-4cd8-9201-3e4ba27a6085",
   "metadata": {},
   "source": [
    "#### Extracts selected columns from a dict, validates timestamps, sorts chronologically,\n",
    "    and filters by time-of-day window.\n",
    "\n",
    "    Parameters:\n",
    "    - source_dict: dict — your working dict (e.g. ib970cln)\n",
    "    - column_group: list — list of column names to extract\n",
    "    - timestamp_col: str — name of the timestamp column\n",
    "    - start_time: datetime.time — lower bound for time-of-day filter\n",
    "    - end_time: datetime.time — upper bound for time-of-day filter\n",
    "\n",
    "    Returns:\n",
    "    - df_filtered: pd.DataFrame — cleaned, sorted, and time-window-filtered DataFrame\n",
    "    - df_errors: pd.DataFrame — rows with invalid timestamps\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32bd81b-0c3b-4fd6-80da-76355b978da3",
   "metadata": {},
   "source": [
    "###  Steps going from ib970cln to ib970cl_mrn_grp1\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc75aff-4ed6-4cae-b38f-c319956ef0ac",
   "metadata": {},
   "source": [
    "#### \n",
    "    # Step 1: Extract selected columns\n",
    "    df = pd.DataFrame({col: source_dict[col] for col in column_group if col in source_dict})\n",
    "\n",
    "    # Step 2: Validate timestamp column\n",
    "    if timestamp_col not in df.columns:\n",
    "        raise ValueError(f\"Timestamp column '{timestamp_col}' not found in selected group.\")\n",
    "\n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors=\"coerce\")\n",
    "\n",
    "    # Step 3: Split valid and invalid timestamps\n",
    "    df_valid = df[df[timestamp_col].notna()].copy()\n",
    "    df_errors = df[df[timestamp_col].isna()].copy()\n",
    "\n",
    "    # Step 4: Sort chronologically\n",
    "    df_valid = df_valid.sort_values(timestamp_col).reset_index(drop=True)\n",
    "\n",
    "    # Step 5: Extract time component\n",
    "    df_valid[\"time_only\"] = df_valid[timestamp_col].dt.time\n",
    "\n",
    "    # Step 6: Filter by time-of-day window\n",
    "    df_filtered = df_valid[\n",
    "        (df_valid[\"time_only\"] >= start_time) &\n",
    "        (df_valid[\"time_only\"] <= end_time)\n",
    "    ].copy()\n",
    "\n",
    "    # Step 7: Drop helper column\n",
    "    df_filtered.drop(columns=[\"time_only\"], inplace=True)\n",
    "\n",
    "    return df_filtered, df_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe18ba20-6f3b-4ce8-a741-b483a62cd9e9",
   "metadata": {},
   "source": [
    "### Calling functions to implement the evolution from **ib970cln** to  **ib970cl_mrn_grp1** using the **ib970mrn_grp1_nms**\n",
    "1. def extract_and_filter_by_time_window(**ib970cln**, **ib970mrn_grp1_nms**)\n",
    "2. from **sys_funcs.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5764ce-1332-42c2-86c3-1afe9a12d232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify \n",
    "# ib970cln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625ed734-fb98-4048-a1bc-ca44d94211b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify \n",
    "# \n",
    "ib970mrn_grp1_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2d35a-0367-43c7-a08a-3c8f8030efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function uses THIS FUNCTION stored in \"sys_funcS.py to use the the ib970mrn_grp1_nms to collect the right data_cols from ib970cln for ib970mrn_grp1\n",
    "#  IT THEN PERFORMS THE 7 STEPS TO RESULT IN       \"ib970mrn_grp1 \n",
    "column_group = ib970mrn_grp1_nms\n",
    "# no_ no_no_no_no_The following will take morning an eve reading Put # in front below to to do only mnr\n",
    "# no_no_no_no_no_no_bring   to this line to do only mrn reads column_group = df_ib970cln_col_nms\n",
    "#\n",
    "source_dict = ib970cln\n",
    "# select the times to get mrn or all\n",
    "# ib970mrn_grp1,df_errors = extract_and_filter_by_time_window(source_dict, column_group, timestamp_col=\"Cleaned_Timestamp\",start_time=time(4, 0), end_time=time(11, 0))  #mrn\n",
    "ib970mrn_grp1,df_errors = extract_and_filter_by_time_window(source_dict, column_group, timestamp_col=\"Cleaned_Timestamp\",start_time=time(0, 0), end_time=time(23,59))    #all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cecdaf4-36f9-40dc-9622-63a1ae472448",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# verify\n",
    "#\n",
    "# ib970mrn_grp1 #[\"dtv\"] #this is the final result \"ib970mrn_grp1\"`\n",
    "# ib970cln #[\"160. 50kHz-Whole Body Phase Angle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0245151e-1d55-44bb-8995-f74c084d0832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify to see the rejected teststamps\n",
    "# df_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c3755-d3b9-45bd-89fd-2fc5cdb5008c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### ib970mrn_grp1[\"14. Weight\"]    \n",
    "pd.set_option('display.max_rows', None)\n",
    "ib970mrn_grp1[['dtv', 'Cleaned_Timestamp','14. Weight']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab67251-db6b-4a86-90a9-988f3c3a8e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "kind_of(ib970mrn_grp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51a6318-8f41-4f1b-943c-5c13028aa48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ib970mrn_grp1 to a file\n",
    "with open(\"ib970mrn_grp1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ib970mrn_grp1, f)\n",
    "print(\" ib970mrn_grp1 is saved to pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcc0890-cc2c-45bd-a59c-a216647d2c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cbd798-aa7e-4f57-846f-c5ba11fc4251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "# ib970mrn_grp1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0c6c37-85cf-4f55-a66f-e92bcf585651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ib970mrn_grp1_nms to a file\n",
    "with open(\"ib970mrn_grp1_nms.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ib970mrn_grp1_nms, f)\n",
    "print(\" ib970mrn_grp1_nms is saved to pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156c94f-a673-466f-9b65-a1581f3854a7",
   "metadata": {},
   "source": [
    "# The rest are spares and duplicates >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49711d8b-dd9c-46a0-8668-06fbc130b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"all cells run ip970mrn_grp1 in pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebefcab6-8f61-45d1-9295-615ac00b8c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7c037e-c6ed-49a2-b750-cc1bca6978d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c61b51-fa20-438d-997a-0227c4e36b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0263ab68-2f82-45f1-adbd-32961a2161e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7ef1bd-3b80-402c-9d08-5dc75ba688e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
